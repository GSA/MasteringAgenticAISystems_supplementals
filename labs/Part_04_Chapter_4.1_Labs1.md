## Hands-On Lab: Complete Agent Profiling and Optimization Workflow

This hands-on lab guides you through the complete profiling and optimization lifecycle for a production ReAct agent, from baseline performance measurement through systematic optimization and SLA validation. You will profile an unoptimized agent using Nsight Systems to identify bottlenecks, apply TensorRT-LLM optimizations including quantization and paged KV cache, implement asynchronous tool execution to eliminate GPU idle time, use Model Analyzer to discover optimal Triton configurations, and validate 5× throughput improvements against production service-level objectives.

### Lab Objectives and Prerequisites

Completing this lab develops practical profiling skills that transfer directly to production agent deployments. You will learn to interpret Nsight Systems timelines revealing GPU idle periods during tool execution, translate profiling observations into targeted optimization strategies, measure quantization impacts on both performance and model quality, and validate that optimized configurations meet latency and throughput SLA requirements. The lab emphasizes systematic profiling methodology rather than arbitrary performance tuning, teaching diagnostic approaches that identify root causes before applying solutions.

Prerequisites include access to an NVIDIA GPU (A100, H100, or RTX 4090 recommended, though RTX 3090 or 4090 suffice for educational purposes), Docker with GPU support configured (nvidia-docker2 or NVIDIA Container Toolkit), and approximately 50GB disk space for model checkpoints, TensorRT engines, and profiling data. Basic Python familiarity enables understanding the ReAct agent implementation, though the lab provides complete code requiring no modifications. Conceptual understanding of GPU profiling, batching, and quantization from previous sections prepares you to interpret results, but the lab includes sufficient explanation to support hands-on learners who prefer practical experience before theoretical depth.

### Environment Setup and Model Preparation

Lab environment setup clones the agent profiling repository, pulls required NVIDIA containers for Triton Inference Server and Nsight Systems, downloads the Llama-3.1-8B-Instruct model, and creates workspace directories for profiling results and TensorRT engines. The setup process authenticates with Hugging Face using the provided read-only token to access the gated Llama model, downloads the 15GB model checkpoint to local storage, and initializes directory structure separating profiles, engines, and analysis results for organizational clarity.

Docker container versions use Triton 24.12 with TensorRT-LLM Python bindings for model optimization and inference serving, and Nsight Systems 2025.5.1 for timeline profiling with full CUDA trace support. The lab employs containerized tooling rather than local installations to ensure reproducible environments that match production deployment stacks, eliminating "works on my machine" variability from local CUDA versions, library conflicts, and system configuration differences.

Model download uses the Hugging Face CLI with programmatic token authentication, storing the complete Llama-3.1-8B checkpoint including model weights, tokenizer vocabulary, and configuration files in a local directory mounted into inference containers. This checkpoint serves as the source for TensorRT-LLM engine compilation applying various optimization strategies that the lab compares for performance and quality impacts.

### Task 1: Baseline Profiling and Bottleneck Identification

The baseline ReAct agent implementation uses standard PyTorch with Hugging Face Transformers, representing typical unoptimized deployment before applying TensorRT-LLM or inference serving optimizations. The agent loads Llama-3.1-8B in FP16 precision with automatic device placement, implements synchronous tool calls that block during API requests, and processes questions sequentially without batching or concurrent execution. This baseline establishes performance metrics that optimization efforts will improve, creating a fair comparison showing the impact of each optimization technique.

Agent execution follows the classic ReAct pattern across five reasoning steps, generating thoughts about how to answer questions, extracting action function calls from generated text, executing simulated search tool calls with realistic 200-millisecond latency, and processing observations to continue reasoning or provide final answers. The synchronous implementation waits for each tool call to complete before proceeding, creating GPU idle periods visible in profiling timelines. Benchmark measurement processes 30 questions through the agent sequentially, recording total execution time, average latency per question, and throughput in questions per second.

Nsight Systems profiling captures the complete execution timeline including CUDA kernel launches, NVTX annotations marking reasoning steps and tool calls, memory transfers between host and device, and CPU thread activity during preprocessing. The profiling command specifies trace collection for CUDA operations, NVTX ranges, and OS runtime events, enables CUDA memory usage tracking, and writes results to a binary report file for GUI analysis. Baseline performance achieves approximately 0.34 questions per second throughput with 2,973-millisecond average latency, indicating substantial optimization opportunity.

Timeline analysis in the Nsight Systems GUI reveals the root cause of poor performance—GPU idle periods during synchronous tool calls where compute resources sit unused waiting for external API responses. The timeline shows alternating patterns of GPU-active inference (thought generation and observation processing) and GPU-idle periods (tool execution), with idle time consuming roughly 40-50% of total execution time. This observation identifies tool call latency as the primary bottleneck requiring asynchronous execution to overlap network I/O with GPU computation.

### Task 2: TensorRT-LLM Optimization and Async Execution

TensorRT-LLM engine compilation optimizes the baseline PyTorch model through kernel fusion, quantization, and memory layout transformations that improve inference efficiency. The lab builds two TensorRT engines for comparison—an FP16 baseline preserving original precision to isolate non-quantization optimizations, and an FP8 optimized engine applying aggressive quantization for maximum throughput. Both engines enable gemm_plugin for optimized matrix multiplication kernels, configure maximum batch size 8 or 16 to support concurrent request processing, and specify maximum input/output sequence lengths matching expected workload patterns.

FP8 quantization reduces model weight and activation precision from 16-bit floating point to 8-bit, halving memory bandwidth requirements and enabling twice as many operations per second on GPUs with FP8 tensor core support. The quantization process uses calibration data to determine optimal scaling factors that minimize accuracy loss, typically achieving less than 2% degradation on reasoning benchmarks while delivering 2-4× speedup. The lab enables paged KV cache during FP8 engine compilation, applying block-based memory management that reduces fragmentation and supports higher batch sizes by avoiding contiguous allocation requirements.

Asynchronous tool execution refactors the baseline agent to overlap tool calls with subsequent inference operations rather than blocking during external API requests. The optimized implementation uses Python's asyncio framework with thread pool executors, submitting tool calls to background threads while immediately proceeding to process the next question's thought generation. This pipeline parallelism hides tool latency behind GPU computation—while request A waits for search results, the GPU processes request B's inference. The async refactoring introduces concurrency complexity requiring careful future management and state tracking, but the performance benefits justify the engineering investment.

Profiling the optimized agent with TensorRT-LLM FP8 and async tools reveals dramatic improvements: total execution time decreases from 89.2 seconds to 28.4 seconds, throughput increases from 0.34 to 1.06 questions per second (3.1× improvement), and average latency drops from 2,973ms to 947ms (68% reduction). GPU utilization timeline shows mostly continuous kernel execution with minimal idle gaps, confirming the async tool execution successfully eliminated synchronous blocking bottlenecks.

### Task 3: Triton Model Analyzer Configuration Search

Model Analyzer automates the search for optimal Triton Inference Server configurations by systematically testing combinations of instance counts, batch sizes, and dynamic batching parameters while measuring latency and throughput. The lab creates a Triton model repository containing the optimized FP8 TensorRT-LLM engine, configures an initial serving configuration specifying the tensorrt_llm backend and dynamic batching preferences, and launches Model Analyzer to explore the configuration space seeking Pareto-optimal setups.

The automated configuration search varies instance count from 1 to 4 concurrent model instances, tests batch sizes from 1 to 16, adjusts dynamic batching timeouts from 100 microseconds to 100 milliseconds, and profiles each configuration under controlled load patterns. Model Analyzer generates 42 measurements across 8 distinct configurations, recording throughput, p50/p95/p99 latency percentiles, GPU memory consumption, and compute utilization for each setup. Analysis identifies configuration 5 as the throughput leader achieving 1,850 requests per second with p99 latency 425ms and 28GB memory footprint, while configuration 3 offers a latency-optimized alternative with p99 latency 245ms and 1,520 RPS throughput.

Configuration selection depends on service-level objectives—deployments requiring p99 latency below 300ms choose configuration 3 despite lower throughput, while throughput-maximizing batch workflows select configuration 5 accepting higher latency for 22% more requests per second. The Model Analyzer comparison table quantifies the latency-throughput tradeoff across the Pareto frontier, enabling evidence-based configuration decisions aligned with business requirements rather than arbitrary performance tuning.

Deploying the optimal configuration copies the selected Triton config.pbtxt file to the production model repository and starts Triton Inference Server with the optimized parameters. The deployment serves inference requests through Triton's HTTP and gRPC endpoints with dynamic batching, concurrent instance execution, and TensorRT-LLM acceleration, delivering the 1,850 RPS throughput measured during Model Analyzer profiling.

### Lab Validation: Performance Improvement and SLO Compliance

Final benchmark comparison measures performance across the optimization progression from baseline PyTorch through TensorRT-LLM to fully optimized Triton deployment. Baseline throughput of 0.34 requests per second increases to 0.68 RPS with TensorRT-LLM FP16 (2.0× improvement), reaches 1.06 RPS with async tools and FP8 quantization (3.1× total improvement), and achieves 1.85 RPS with optimized Triton configuration (5.4× final improvement). Latency p50 decreases from 2,850ms baseline to 485ms optimized (83% reduction), while p99 latency improves from 3,180ms to 580ms (82% reduction). GPU utilization increases from 28% baseline to 86% optimized, demonstrating successful elimination of idle periods through async execution and batching.

Memory efficiency shows surprising patterns—TensorRT-LLM FP8 reduces memory from 18.2GB baseline to 9.8GB (46% reduction), but the optimized Triton configuration increases memory to 14.2GB to support higher batch sizes and concurrent instances. This memory increase represents intentional resource allocation to maximize throughput rather than inefficiency, trading available memory for serving capacity that business metrics prioritize.

Service-level objective validation checks the optimized system against production requirements: p50 latency 485ms passes the 500ms target with margin, p99 latency 580ms passes the 1,000ms target comfortably, throughput 1.85 RPS exceeds the 1.5 RPS minimum, and GPU utilization 86% exceeds the 70% efficiency target. All SLO dimensions pass validation, confirming the optimized configuration meets production deployment standards. The systematic optimization process delivers 5.4× throughput improvement and 82% latency reduction while maintaining model quality and SLA compliance, demonstrating the impact of profiling-guided optimization over intuition-based performance tuning.

### Production Profiling Best Practices

Continuous performance monitoring in production deployments instruments agent code with metrics collection tracking request latency, LLM generation time, tool execution time, and token counts for every inference operation. The instrumentation logs performance data to structured files enabling post-deployment analysis without runtime overhead, flushing metrics batches periodically to balance disk I/O costs against data freshness. Prometheus integration exports these metrics through standard monitoring interfaces, enabling Grafana dashboards that visualize latency distributions, throughput trends, and resource utilization over time.

Performance regression detection automatically alerts when production metrics degrade relative to baseline expectations, catching deployment problems or traffic pattern changes before they violate SLOs. The regression detector maintains rolling window statistics of recent latency measurements, comparing current p95 latency against baseline thresholds and triggering alerts when degradation exceeds acceptable bounds (typically 20% regression). Automated alerting integrates with PagerDuty, Slack, or other incident management systems to notify engineering teams of performance problems requiring investigation.

Pre-deployment profiling validates performance under production-like workloads before launch, testing realistic batch sizes, sequence lengths, and traffic patterns that match expected production demand. Stress testing pushes the system to maximum load to verify headroom and identify breaking points, ensuring deployments handle traffic spikes without catastrophic failures. Post-deployment monitoring validates profiling predictions by comparing predicted performance against actual production measurements, building confidence in profiling methodology and identifying any gaps between benchmark environments and production reality.

Monthly re-profiling detects performance drift over time as model updates, library upgrades, or traffic pattern changes gradually degrade efficiency. Scheduled profiling runs capture performance baselines periodically, enabling longitudinal analysis that reveals subtle degradation invisible in daily operational monitoring. This proactive profiling catches problems early when fixes remain simple rather than waiting for SLO violations that indicate severe performance collapse.